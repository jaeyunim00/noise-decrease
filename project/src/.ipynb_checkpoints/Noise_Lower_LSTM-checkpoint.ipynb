{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "class NoiseDataset(Dataset):\n",
    "    def __init__(self, audio_file, sample_rate=16000, frame_size=1024, threshold=0.05):\n",
    "        waveform, orig_sample_rate = torchaudio.load(audio_file)\n",
    "        self.waveform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=sample_rate)(waveform)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.frame_size = frame_size\n",
    "        self.threshold = threshold\n",
    "        self.noise_indices = self._detect_noise(self.waveform)\n",
    "        \n",
    "    def _detect_noise(self, waveform):\n",
    "        noise_indices = []\n",
    "        energy = waveform.pow(2).mean(dim=0)\n",
    "        for i in range(0, waveform.size(1) - self.frame_size, self.frame_size):\n",
    "            frame_energy = energy[i:i+self.frame_size].mean().item()\n",
    "            if frame_energy > self.threshold:\n",
    "                noise_indices.append(i)\n",
    "                \n",
    "        return noise_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noise_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.noise_indices[idx]\n",
    "        end_idx = start_idx + self.frame_size\n",
    "\n",
    "        noisy_segment = self.waveform[:, start_idx:end_idx]\n",
    "        noisy_segment = noisy_segment.unsqueeze(-1)  \n",
    "\n",
    "        return noisy_segment, noisy_segment\n",
    "\n",
    "    def extract_highest_noise_segment(waveform, sample_rate=16000, segment_duration=10):\n",
    "        \"\"\"\n",
    "        가장 높은 소음 구간을 추출하는 정적 메서드.\n",
    "\n",
    "        Parameters:\n",
    "        - waveform (torch.Tensor): 오디오 파형.\n",
    "        - sample_rate (int): 샘플링 속도.\n",
    "        - segment_duration (int): 추출할 구간의 길이 (초).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: 가장 높은 소음 구간의 파형.\n",
    "        \"\"\"\n",
    "        # 에너지 계산\n",
    "        energy = waveform.pow(2).mean(dim=0)\n",
    "\n",
    "        # 구간의 샘플 수 계산\n",
    "        segment_samples = segment_duration * sample_rate\n",
    "\n",
    "        # 최고 에너지를 가진 구간 탐색\n",
    "        max_energy = 0\n",
    "        max_index = 0\n",
    "\n",
    "        for i in range(0, waveform.size(1) - segment_samples, segment_samples):\n",
    "            segment_energy = energy[i:i + segment_samples].sum().item()\n",
    "            if segment_energy > max_energy:\n",
    "                max_energy = segment_energy\n",
    "                max_index = i\n",
    "\n",
    "        # 가장 높은 소음 구간 추출\n",
    "        highest_noise_segment = waveform[:, max_index:max_index + segment_samples]\n",
    "\n",
    "        return highest_noise_segment\n",
    "    \n",
    "class AntiNoiseLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(AntiNoiseLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "def train_model(dataloader, model, criterion, optimizer, num_epochs = 20, validation_loader = None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if inputs.dim() == 2: \n",
    "                inputs = inputs.unsqueeze(1) \n",
    "            elif inputs.dim() == 4:  \n",
    "                inputs = inputs.squeeze(-1) \n",
    "            elif inputs.dim() != 3: \n",
    "                raise ValueError(f\"Unexpected input dimension {inputs.dim()} with shape {inputs.shape}\")\n",
    "\n",
    "            inputs = inputs.float()\n",
    "            targets = -inputs\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        if validation_loader is not None:\n",
    "            val_loss = evaluate_model(validation_loader, model, criterion)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.unsqueeze(-1).float()\n",
    "            targets = targets.unsqueeze(-1).float()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.unsqueeze(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "val = {\n",
    "    'input_size' : 1,\n",
    "    'hidden_size' : 512,\n",
    "    'num_layers' : 2,\n",
    "    'output_size' : 1,\n",
    "    'learning_rate' : 0.01\n",
    "}\n",
    "\n",
    "def load_datasets_from_folder(folder_path, sample_rate=16000, frame_size=1024, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - folder_path (str): .wav 파일이 있는 폴더의 경로\n",
    "    - sample_rate (int): 샘플링 속도\n",
    "    - frame_size (int): 프레임 크기\n",
    "    - threshold (float): 소음 감지 임계값\n",
    "    - segment_duration (int): 추출할 소음 구간의 길이 (초)\n",
    "\n",
    "    Returns:\n",
    "    - ConcatDataset: 결합된 데이터셋\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_file_path = os.path.join(folder_path, file_name)\n",
    "            dataset = NoiseDataset(audio_file_path, sample_rate, frame_size, threshold)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "    combined_dataset = ConcatDataset(datasets)\n",
    "    return combined_dataset\n",
    "\n",
    "folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Training/Sound'\n",
    "combined_dataset = load_datasets_from_folder(folder_path)\n",
    "\n",
    "dataloader = DataLoader(combined_dataset, batch_size = 64, shuffle = True, pin_memory = True)\n",
    "\n",
    "anti_noise_model = AntiNoiseLSTM(val['input_size'], val['hidden_size'], val['num_layers'], val['output_size'])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(anti_noise_model.parameters(), lr = val['learning_rate'])\n",
    "\n",
    "train_model(dataloader, anti_noise_model, criterion, optimizer)\n",
    "\n",
    "def evaluate_and_plot_with_dataloader(model, dataloader):\n",
    "    model.eval()\n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = inputs.squeeze(-1).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "        input_waveform = inputs[0].cpu().numpy().flatten()\n",
    "        output_waveform = outputs[0].cpu().numpy().flatten()\n",
    "\n",
    "        print(f\"Input waveform shape: {input_waveform.shape}\")\n",
    "        print(f\"Output waveform shape: {output_waveform.shape}\")\n",
    "        print(f\"Input waveform first 10 values: {input_waveform[:10]}\")\n",
    "        print(f\"Output waveform first 10 values: {output_waveform[:10]}\")\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(input_waveform, label='Original Noise')\n",
    "        plt.plot(output_waveform, label='Predicted Anti-Noise', color='orange', alpha=0.7)\n",
    "        plt.title('Original Noise vs Predicted Anti-Noise')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "test_folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Validation/sound'\n",
    "test_combined_dataset = load_datasets_from_folder(test_folder_path)\n",
    "\n",
    "test_dataloader = DataLoader(test_combined_dataset, batch_size = 64, shuffle=True)\n",
    "\n",
    "evaluate_and_plot_with_dataloader(anti_noise_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:2.4.0\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-14.6.1-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\")\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\")\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\")\n",
    "!python -c 'import platform;print(platform.platform())'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 소음 구간 태깅\n",
    "\n",
    "2. 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseDataset(Dataset):\n",
    "    def __init__(self, audio_file, sample_rate=16000, frame_size=1024, threshold=0.05):\n",
    "        waveform, orig_sample_rate = torchaudio.load(audio_file)\n",
    "        self.waveform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=sample_rate)(waveform)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.frame_size = frame_size\n",
    "        self.threshold = threshold\n",
    "        self.noise_indices = self._detect_noise(self.waveform)\n",
    "        \n",
    "    def _detect_noise(self, waveform):\n",
    "        noise_indices = []\n",
    "        energy = waveform.pow(2).mean(dim=0)\n",
    "        for i in range(0, waveform.size(1) - self.frame_size, self.frame_size):\n",
    "            frame_energy = energy[i:i+self.frame_size].mean().item()\n",
    "            if frame_energy > self.threshold:\n",
    "                noise_indices.append(i)\n",
    "                \n",
    "        return noise_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noise_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.noise_indices[idx]\n",
    "        end_idx = start_idx + self.frame_size\n",
    "\n",
    "        noisy_segment = self.waveform[:, start_idx:end_idx]\n",
    "        noisy_segment = noisy_segment.unsqueeze(-1)  \n",
    "\n",
    "        return noisy_segment, noisy_segment\n",
    "\n",
    "    def extract_highest_noise_segment(waveform, sample_rate=16000, segment_duration=10):\n",
    "        \"\"\"\n",
    "        가장 높은 소음 구간을 추출하는 정적 메서드.\n",
    "\n",
    "        Parameters:\n",
    "        - waveform (torch.Tensor): 오디오 파형.\n",
    "        - sample_rate (int): 샘플링 속도.\n",
    "        - segment_duration (int): 추출할 구간의 길이 (초).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: 가장 높은 소음 구간의 파형.\n",
    "        \"\"\"\n",
    "        # 에너지 계산\n",
    "        energy = waveform.pow(2).mean(dim=0)\n",
    "\n",
    "        # 구간의 샘플 수 계산\n",
    "        segment_samples = segment_duration * sample_rate\n",
    "\n",
    "        # 최고 에너지를 가진 구간 탐색\n",
    "        max_energy = 0\n",
    "        max_index = 0\n",
    "\n",
    "        for i in range(0, waveform.size(1) - segment_samples, segment_samples):\n",
    "            segment_energy = energy[i:i + segment_samples].sum().item()\n",
    "            if segment_energy > max_energy:\n",
    "                max_energy = segment_energy\n",
    "                max_index = i\n",
    "\n",
    "        # 가장 높은 소음 구간 추출\n",
    "        highest_noise_segment = waveform[:, max_index:max_index + segment_samples]\n",
    "\n",
    "        return highest_noise_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntiNoiseLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(AntiNoiseLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, criterion, optimizer, num_epochs = 20, validation_loader = None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            if inputs.dim() == 2: \n",
    "                inputs = inputs.unsqueeze(1) \n",
    "            elif inputs.dim() == 4:  \n",
    "                inputs = inputs.squeeze(-1) \n",
    "            elif inputs.dim() != 3: \n",
    "                raise ValueError(f\"Unexpected input dimension {inputs.dim()} with shape {inputs.shape}\")\n",
    "\n",
    "            inputs = inputs.float()\n",
    "            targets = -inputs\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        if validation_loader is not None:\n",
    "            val_loss = evaluate_model(validation_loader, model, criterion)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs = inputs.unsqueeze(-1).float()\n",
    "            targets = targets.unsqueeze(-1).float()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.unsqueeze(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파라미터 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = {\n",
    "    'input_size' : 1024,\n",
    "    'hidden_size' : 512,\n",
    "    'num_layers' : 2,\n",
    "    'output_size' : 1024,\n",
    "    'learning_rate' : 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 24) (413871890.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 11\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 24)\n"
     ]
    }
   ],
   "source": [
    "def load_datasets_from_folder(folder_path, sample_rate=16000, frame_size=1024, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - folder_path (str): .wav 파일이 있는 폴더의 경로\n",
    "    - sample_rate (int): 샘플링 속도\n",
    "    - frame_size (int): 프레임 크기\n",
    "    - threshold (float): 소음 감지 임계값\n",
    "    - segment_duration (int): 추출할 소음 구간의 길이 (초)\n",
    "\n",
    "    Returns:\n",
    "    - ConcatDataset: 결합된 데이터셋\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_file_path = os.path.join(folder_path, file_name)\n",
    "            dataset = NoiseDataset(audio_file_path, sample_rate, frame_size, threshold)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "    combined_dataset = ConcatDataset(datasets)\n",
    "    return combined_dataset\n",
    "\n",
    "folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Training/Sound'\n",
    "combined_dataset = load_datasets_from_folder(folder_path)\n",
    "\n",
    "dataloader = DataLoader(combined_dataset, batch_size = 64, shuffle = True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델, 손실 함수, 옵티마이저 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_noise_model = AntiNoiseLSTM(val['input_size'], val['hidden_size'], val['num_layers'], val['output_size'])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(anti_noise_model.parameters(), lr = val['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(dataloader, anti_noise_model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "anti_noise_model.to('cpu')\n",
    "\n",
    "with open('Noise_Lower_LSTM_vol_1.pkl', 'wb') as f:\n",
    "    pickle.dump(anti_noise_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_with_dataloader(model, dataloader):\n",
    "    model.eval()\n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = inputs.squeeze(-1).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "        input_waveform = inputs[0].cpu().numpy().flatten()\n",
    "        output_waveform = outputs[0].cpu().numpy().flatten()\n",
    "\n",
    "        print(f\"Input waveform shape: {input_waveform.shape}\")\n",
    "        print(f\"Output waveform shape: {output_waveform.shape}\")\n",
    "        print(f\"Input waveform first 10 values: {input_waveform[:10]}\")\n",
    "        print(f\"Output waveform first 10 values: {output_waveform[:10]}\")\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(input_waveform, label='Original Noise')\n",
    "        plt.plot(output_waveform, label='Predicted Anti-Noise', color='orange', alpha=0.7)\n",
    "        plt.title('Original Noise vs Predicted Anti-Noise')\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Validation/sound'\n",
    "test_combined_dataset = load_datasets_from_folder(test_folder_path)\n",
    "\n",
    "test_dataloader = DataLoader(test_combined_dataset, batch_size = 64, shuffle=True)\n",
    "\n",
    "evaluate_and_plot_with_dataloader(anti_noise_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실시간 오디오 테스트"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AITECH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
