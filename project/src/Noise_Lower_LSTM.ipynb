{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyaudio \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\")\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\")\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\")\n",
    "!python -c 'import platform;print(platform.platform())'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 소음 구간 태깅\n",
    "\n",
    "2. 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseDataset(Dataset):\n",
    "    def __init__(self, audio_file, sample_rate = 44100, frame_size = 1024, threshold = 0.05):\n",
    "        waveform, orig_sample_rate = torchaudio.load(audio_file)\n",
    "        self.waveform = torchaudio.transforms.Resample(orig_freq=orig_sample_rate, new_freq=sample_rate)(waveform)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.frame_size = frame_size\n",
    "        self.threshold = threshold\n",
    "        self.noise_indices = self._detect_noise(self.waveform)\n",
    "        \n",
    "    def _detect_noise(self, waveform):\n",
    "        noise_indices = []\n",
    "        energy = waveform.pow(2).mean(dim=0)\n",
    "        for i in range(0, waveform.size(1) - self.frame_size, self.frame_size):\n",
    "            frame_energy = energy[i:i+self.frame_size].mean().item()\n",
    "            if frame_energy > self.threshold:\n",
    "                noise_indices.append(i)\n",
    "        return noise_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noise_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = self.noise_indices[idx]\n",
    "        end_idx = start_idx + self.frame_size\n",
    "        noisy_segment = self.waveform[:, start_idx:end_idx]\n",
    "        return noisy_segment, noisy_segment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntiNoiseLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(AntiNoiseLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(dataloader, model, criterion, optimizer, num_epochs = 10, validation_loader = None):\n",
    "#     train_losses = []\n",
    "#     val_losses = []\n",
    "\n",
    "#     model.to(device)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         for i, (inputs, targets) in enumerate(dataloader):\n",
    "#             inputs = inputs.to(device)\n",
    "#             targets = targets.to(device)\n",
    "\n",
    "#             if inputs.dim() == 2: \n",
    "#                 inputs = inputs.unsqueeze(1) \n",
    "#             elif inputs.dim() == 4:  \n",
    "#                 inputs = inputs.squeeze(-1) \n",
    "#             elif inputs.dim() != 3: \n",
    "#                 raise ValueError(f\"Unexpected input dimension {inputs.dim()} with shape {inputs.shape}\")\n",
    "\n",
    "#             inputs = inputs.float()\n",
    "#             targets = -inputs\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, targets)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             if (i + 1) % 10 == 0:\n",
    "#                 print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "#         epoch_loss = running_loss / len(dataloader)\n",
    "#         train_losses.append(epoch_loss)\n",
    "\n",
    "#         if validation_loader is not None:\n",
    "#             val_loss = evaluate_model(validation_loader, model, criterion)\n",
    "#             val_losses.append(val_loss)\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "#         else:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "#     return train_losses, val_losses\n",
    "\n",
    "# def evaluate_model(dataloader, model, criterion):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in dataloader:\n",
    "#             inputs = inputs.unsqueeze(-1).float()\n",
    "#             targets = targets.unsqueeze(-1).float()\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "#             outputs = outputs.unsqueeze(-1)\n",
    "#             loss = criterion(outputs, targets)\n",
    "#             total_loss += loss.item()\n",
    "    \n",
    "#     return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vol2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, criterion, optimizer, num_epochs = 10, validation_loader=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            if inputs.dim() == 4:\n",
    "                # 입력 데이터가 4D인 경우, (batch, channels, seq_len, input_size)\n",
    "                batch_size, channels, seq_len, input_size = inputs.shape\n",
    "                # LSTM이 기대하는 입력 형태로 변환\n",
    "                inputs = inputs.view(batch_size * channels, seq_len, input_size).float()\n",
    "            elif inputs.dim() == 3:\n",
    "                # 입력 데이터가 이미 3D인 경우, (batch, seq_len, input_size)\n",
    "                inputs = inputs.float()\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected input dimensions: {inputs.shape}\")\n",
    "\n",
    "            # 타겟 데이터도 적절히 float으로 변환\n",
    "            targets = -inputs\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        if validation_loader is not None:\n",
    "            val_loss = evaluate_model(validation_loader, model, criterion)\n",
    "            val_losses.append(val_loss)\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        else:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            inputs = inputs.unsqueeze(-1).float()\n",
    "            targets = -inputs\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "    if val_losses:\n",
    "        plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloader, model, criterion, optimizer, num_epochs = 10, validation_loader=None):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            if inputs.dim() == 4:\n",
    "                # 입력 데이터가 4D인 경우, (batch, channels, seq_len, input_size)\n",
    "                batch_size, channels, seq_len, input_size = inputs.shape\n",
    "                # LSTM이 기대하는 입력 형태로 변환\n",
    "                inputs = inputs.view(batch_size * channels, seq_len, input_size).float()\n",
    "            elif inputs.dim() == 3:\n",
    "                # 입력 데이터가 이미 3D인 경우, (batch, seq_len, input_size)\n",
    "                inputs = inputs.float()\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected input dimensions: {inputs.shape}\")\n",
    "\n",
    "            # 타겟 데이터도 적절히 float으로 변환\n",
    "            targets = -inputs\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return train_losses\n",
    "\n",
    "def evaluate_model(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # 입력 데이터 차원에 따른 전처리\n",
    "            if inputs.dim() == 4:\n",
    "                # 4D 텐서인 경우: (batch_size, channels, seq_len, input_size)\n",
    "                batch_size, channels, seq_len, input_size = inputs.shape\n",
    "                inputs = inputs.view(batch_size * channels, seq_len, input_size).float()  # 3D로 변환\n",
    "            elif inputs.dim() == 3:\n",
    "                # 3D 텐서인 경우: (batch_size, seq_len, input_size)\n",
    "                inputs = inputs.float()\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected input dimensions: {inputs.shape}\")\n",
    "\n",
    "            targets = -inputs\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "    if val_losses:\n",
    "        plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파라미터 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = {\n",
    "    'input_size' : 1024,\n",
    "    'hidden_size' : 512,\n",
    "    'num_layers' : 2,\n",
    "    'output_size' : 1024,\n",
    "    'learning_rate' : 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets_from_folder(folder_path, sample_rate=44100, frame_size=1024, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - folder_path (str): .wav 파일이 있는 폴더의 경로\n",
    "    - sample_rate (int): 샘플링 속도\n",
    "    - frame_size (int): 프레임 크기\n",
    "    - threshold (float): 소음 감지 임계값\n",
    "    - segment_duration (int): 추출할 소음 구간의 길이 (초)\n",
    "\n",
    "    Returns:\n",
    "    - ConcatDataset: 결합된 데이터셋\n",
    "    \"\"\"\n",
    "    datasets = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".wav\"):\n",
    "            audio_file_path = os.path.join(folder_path, file_name)\n",
    "            # print(f\"Loading file: {audio_file_path}\") \n",
    "            dataset = NoiseDataset(audio_file_path, sample_rate, frame_size, threshold)\n",
    "            datasets.append(dataset)\n",
    "\n",
    "    combined_dataset = ConcatDataset(datasets)\n",
    "    return combined_dataset\n",
    "\n",
    "folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Preclean_Data/Training'\n",
    "combied_dataset = load_datasets_from_folder(folder_path)\n",
    "\n",
    "dataloader = DataLoader(combined_dataset, batch_size = 64, shuffle = True, pin_memory = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델, 손실 함수, 옵티마이저 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_noise_model = AntiNoiseLSTM(val['input_size'], val['hidden_size'], val['num_layers'], val['output_size'])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(anti_noise_model.parameters(), lr = val['learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train_model(dataloader, anti_noise_model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증 데이터 호출 및 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Preclean_Data/Validation'\n",
    "test_combined_dataset = load_datasets_from_folder(test_folder_path)\n",
    "\n",
    "val_dataloader = DataLoader(test_combined_dataset, batch_size = 64, shuffle=True)\n",
    "\n",
    "val_loss = evaluate_model(val_dataloader, anti_noise_model, criterion)\n",
    "val_losses = [val_loss] * len(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "anti_noise_model.to('cpu')\n",
    "\n",
    "with open('Noise_Lower_LSTM_vol_2.pkl', 'wb') as f:\n",
    "    pickle.dump(anti_noise_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anti_noise_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 첫번째 배치에 대한 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_with_one_batch_dataloader(model, dataloader):\n",
    "    model.eval()\n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.to(device).squeeze(-1).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "        input_waveform = inputs[0].cpu().numpy().flatten()\n",
    "        output_waveform = outputs[0].cpu().numpy().flatten()\n",
    "        combined_waveform = input_waveform + output_waveform\n",
    "\n",
    "        original_noise = input_waveform\n",
    "        predicted_anti_noise = output_waveform\n",
    "\n",
    "        original_energy = np.sum(original_noise ** 2)\n",
    "\n",
    "        combined_waveform = original_noise + predicted_anti_noise\n",
    "        residual_energy = np.sum(combined_waveform ** 2)\n",
    "\n",
    "        noise_reduction_percent = (1 - residual_energy / original_energy) * 100\n",
    "\n",
    "        print(f\"소음 감소율: {noise_reduction_percent:.2f}%\")\n",
    "\n",
    "\n",
    "        print(f\"Input waveform shape: {input_waveform.shape}\")\n",
    "        print(f\"Output waveform shape: {output_waveform.shape}\")\n",
    "        print(f\"Input waveform first 10 values: {input_waveform[:10]}\")\n",
    "        print(f\"Output waveform first 10 values: {output_waveform[:10]}\")\n",
    "        print(f\"Combined waveform first 10 values: {combined_waveform[:10]}\")\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(input_waveform, label='Original Noise')\n",
    "        plt.title('Original Noise')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(output_waveform, label='Predicted Anti-Noise', color='orange', alpha=0.7)\n",
    "        plt.title('Predicted Anti-Noise')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(combined_waveform, label='Combined Waveform', color='green', alpha=0.7)\n",
    "        plt.title('Combined Waveform (Noise + Anti-Noise)')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 배치에 대한 그래프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_with_all_batch_dataloader(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    for batch_idx, (inputs, _) in enumerate(dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = inputs.squeeze(-1).float()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        for i in range(inputs.size(0)):\n",
    "            input_waveform = inputs[i].cpu().numpy().flatten()\n",
    "            output_waveform = outputs[i].cpu().numpy().flatten()\n",
    "            combined_waveform = input_waveform + output_waveform\n",
    "\n",
    "            print(f\"Batch {batch_idx+1}, Sample {i+1}\")\n",
    "            print(f\"Input waveform shape: {input_waveform.shape}\")\n",
    "            print(f\"Output waveform shape: {output_waveform.shape}\")\n",
    "            print(f\"Input waveform first 10 values: {input_waveform[:10]}\")\n",
    "            print(f\"Output waveform first 10 values: {output_waveform[:10]}\")\n",
    "            print(f\"Combined waveform first 10 values: {combined_waveform[:10]}\")\n",
    "\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            plt.subplot(3, 1, 1)\n",
    "            plt.plot(input_waveform, label='Original Noise')\n",
    "            plt.title('Original Noise')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 1, 2)\n",
    "            plt.plot(output_waveform, label='Predicted Anti-Noise', color='orange', alpha=0.7)\n",
    "            plt.title('Predicted Anti-Noise')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.subplot(3, 1, 3)\n",
    "            plt.plot(combined_waveform, label='Combined Waveform', color='green', alpha=0.7)\n",
    "            plt.title('Combined Waveform (Noise + Anti-Noise)')\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Preclean_Data/Validation'\n",
    "test_combined_dataset = load_datasets_from_folder(test_folder_path)\n",
    "\n",
    "test_dataloader = DataLoader(test_combined_dataset, batch_size = 64, shuffle=True)\n",
    "\n",
    "# evaluate_and_plot_with_one_batch_dataloader(anti_noise_model, test_dataloader)\n",
    "evaluate_and_plot_with_all_batch_dataloader(anti_noise_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실시간 오디오 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_audio_processing(model, input_size=1, sample_rate=44100, chunk_size=1024):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream_in = p.open(format=pyaudio.paFloat32,\n",
    "                       channels=1,\n",
    "                       rate=sample_rate,\n",
    "                       input=True,\n",
    "                       frames_per_buffer=chunk_size)\n",
    "\n",
    "    stream_out = p.open(format=pyaudio.paFloat32,\n",
    "                        channels=1,\n",
    "                        rate=sample_rate,\n",
    "                        output=True)\n",
    "\n",
    "    print(\"실시간 오디오 처리를 시작합니다...\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = stream_in.read(chunk_size)\n",
    "            waveform = torch.tensor(np.frombuffer(data, dtype=np.float32)).unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                anti_noise = model(waveform)\n",
    "\n",
    "            anti_noise_data = anti_noise.squeeze().numpy().astype(np.float32).tobytes()\n",
    "            stream_out.write(anti_noise_data)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"실시간 오디오 처리를 종료합니다.\")\n",
    "    finally:\n",
    "        stream_in.stop_stream()\n",
    "        stream_in.close()\n",
    "        stream_out.stop_stream()\n",
    "        stream_out.close()\n",
    "        p.terminate()\n",
    "\n",
    "real_time_audio_processing(anti_noise_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
