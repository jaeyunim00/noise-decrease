{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:2.4.0\n",
      "MPS 장치를 지원하도록 build 되었는지: True\n",
      "MPS 장치가 사용 가능한지: True\n",
      "macOS-14.6.1-arm64-arm-64bit\n"
     ]
    }
   ],
   "source": [
    "print (f\"PyTorch version:{torch.__version__}\")\n",
    "print(f\"MPS 장치를 지원하도록 build 되었는지: {torch.backends.mps.is_built()}\")\n",
    "print(f\"MPS 장치가 사용 가능한지: {torch.backends.mps.is_available()}\")\n",
    "!python -c 'import platform;print(platform.platform())'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import wavfile\n",
    "import soundfile as sf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Training/Sound'\n",
    "file_list = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "\n",
    "random.shuffle(file_list)\n",
    "\n",
    "half_length = len(file_list) // 100\n",
    "file_list = file_list[:half_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_loudest_segment(data, sample_rate, segment_duration=5):\n",
    "#     segment_length = int(sample_rate * segment_duration)\n",
    "#     max_amplitude = 0\n",
    "#     loudest_segment = None\n",
    "    \n",
    "#     for i in range(0, len(data) - segment_length, sample_rate):\n",
    "#         segment = data[i:i + segment_length]\n",
    "#         amplitude = np.sum(segment ** 2)\n",
    "#         if amplitude > max_amplitude:\n",
    "#             max_amplitude = amplitude\n",
    "#             loudest_segment = segment\n",
    "    \n",
    "#     return loudest_segment\n",
    "\n",
    "# all_segments = []\n",
    "\n",
    "# for file_path in file_list:\n",
    "#     data, sample_rate = sf.read(file_path)\n",
    "#     loudest_segment = find_loudest_segment(data, sample_rate)\n",
    "#     if loudest_segment is not None:\n",
    "#         all_segments.append(loudest_segment)\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# all_segments = [scaler.fit_transform(segment.reshape(-1, 1)).flatten() for segment in all_segments]\n",
    "# all_data = np.concatenate(all_segments)\n",
    "def find_loudest_segment(data, sample_rate, segment_duration=5):\n",
    "    segment_length = int(sample_rate * segment_duration)\n",
    "    max_amplitude = 0\n",
    "    loudest_segment = None\n",
    "\n",
    "    for i in range(0, len(data) - segment_length, sample_rate):\n",
    "        segment = data[i:i + segment_length]\n",
    "        amplitude = np.sum(segment ** 2)\n",
    "        if amplitude > max_amplitude:\n",
    "            max_amplitude = amplitude\n",
    "            loudest_segment = segment\n",
    "\n",
    "    return loudest_segment\n",
    "\n",
    "all_segments = []\n",
    "\n",
    "for file_path in file_list:\n",
    "    data, sample_rate = sf.read(file_path)\n",
    "    if data.ndim > 1:  # 다차원 배열인 경우 (스테레오)\n",
    "        data = data.mean(axis=1)  # 스테레오 데이터를 모노로 변환\n",
    "    loudest_segment = find_loudest_segment(data, sample_rate)\n",
    "    if loudest_segment is not None:\n",
    "        all_segments.append(loudest_segment)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "all_segments = [scaler.fit_transform(segment.reshape(-1, 1)).flatten() for segment in all_segments if segment.size > 0]\n",
    "\n",
    "min_length = min(len(segment) for segment in all_segments)  # 가장 짧은 세그먼트 길이 찾기\n",
    "all_segments = [segment[:min_length] for segment in all_segments]  # 모든 세그먼트를 동일한 길이로 자르기\n",
    "\n",
    "all_data = np.concatenate(all_segments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100 \n",
    "input_size = 1        \n",
    "hidden_size = 64      \n",
    "output_size = 1       \n",
    "num_layers = 2       \n",
    "learning_rate = 0.001 \n",
    "num_epochs = 10        \n",
    "batch_size = 32   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시계열 데이터를 슬라이딩 윈도우 기법으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "def create_sequences(data, seq_length):\n",
    "    xs, ys = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:i + seq_length]\n",
    "        y = data[i + seq_length]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "X, y = create_sequences(all_data, sequence_length)\n",
    "X = torch.tensor(X, dtype=torch.float32).view(-1, sequence_length, input_size)\n",
    "y = torch.tensor(y, dtype=torch.float32).view(-1, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 및 데이터로더 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(X, y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNoiseCancelling(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(LSTMNoiseCancelling, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(num_layers, x.size(0), hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 초기화, 손실 함수 및 옵티마이저 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMNoiseCancelling(input_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트 데이터 추출 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder_path = '/Users/junggwonhee/Desktop/programing/오아시스_해커톤/project/data/극한_소리_데이터/Validation/sound'\n",
    "test_file_list = [os.path.join(folder_path, f) for f in os.listdir(test_folder_path) if f.endswith('.wav')]\n",
    "\n",
    "for test_file_path in test_file_list:\n",
    "    sample_rate, data = wavfile.read(test_file_path)\n",
    "    test_data = MinMaxScaler(feature_range=(-1, 1)).fit_transform(test_data.reshape(-1, 1)).flatten()\n",
    "    X_test, _ = create_sequences(test_data, sequence_length)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).view(-1, sequence_length, input_size)\n",
    "    \n",
    "    # 모델 예측\n",
    "    with torch.no_grad():\n",
    "        predicted = model(X_test)\n",
    "        anti_phase = -predicted  # 역위상 생성\n",
    "\n",
    "    print(f\"File: {test_file_path}\")\n",
    "    print(\"Predicted:\", predicted)\n",
    "    print(\"Anti-phase:\", anti_phase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AITECH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
